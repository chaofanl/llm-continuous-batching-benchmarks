#!/usr/bin/env bash

export PORT=1027

random_prompt_lens_mean=$1
response_len=$2
response_range=$3
bs=$4
prompt_range=$6
distribution=${5:-"capped_exponential"}

if [ "$distribution" == "uniform" ]; then
    max_batch_total_tokens=$(( (random_prompt_lens_mean + prompt_range/2 + response_len + response_range/2) * bs ))
else
    max_batch_total_tokens=$(( (random_prompt_lens_mean + prompt_range/2 + response_range) * bs ))
fi

model_name_dir=$7
num_shard=$8
dtype=$9

results_home=$PWD/result
mkdir -p ${results_home}
max_input=$(( random_prompt_lens_mean + prompt_range/2 ))

if [ "$distribution" == "uniform" ]; then
    max_total=$(( random_prompt_lens_mean + prompt_range/2 + response_len + response_range/2))
else
    max_total=$(( random_prompt_lens_mean + prompt_range/2 + response_range))
fi

echo "max_input $max_input max_total_tokens $max_total max_batch_total_tokens $max_batch_total_tokens"

function start_model_server {
    local max_batch_total_tokens=$1
    echo "start_model_server $max_batch_total_tokens"

    ulimit -n 65536 && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
    model_name=$model_name_dir \
    max_batch_total_tokens=$max_batch_total_tokens \
    PORT=$PORT \
    max_input=$max_input \
    max_total=$max_total \
    bs=$bs \
    num_shard=$num_shard \
    dtype=$dtype \
    ../launch_scripts/launch_text_generation_inference \
&
    
    while [ "$(curl -s http://localhost:${PORT}/info | grep $model_name_dir | wc -l)" -eq 0 ]; do
        echo 'ping'
        sleep 1
    done
    echo "model server started on port $PORT, max_batch_total_tokens $max_batch_total_tokens"
}

function kill_model_server {
    echo 'killing model server'
    ps aux | grep 'text-generation-launcher' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-router' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-server' | awk '{print $2}' | xargs kill -9
    wait
}

trap kill_model_server EXIT

start_model_server $max_batch_total_tokens

pushd ..
    ./benchmark_throughput.py \
        --port $PORT \
        --backend HfTextGenerationInference \
        --random_prompt_lens_mean ${random_prompt_lens_mean} \
        --random_prompt_lens_range ${prompt_range} \
        --random_prompt_count 1000 \
        --gen_random_prompts \
        --variable_response_lens_mean ${response_len} \
        --variable_response_lens_range ${response_range} \
        --variable_response_lens_distribution ${distribution} \
        --allow_variable_generation_length \
        --model_path $model_name_dir \
        --max_num_threads $bs \
        --results_filename ${results_home}/tgi_a800_im${random_prompt_lens_mean}_ir${prompt_range}_om${response_len}_or${response_range}_bs${bs}_cn${num_shard}_dt${dtype}.log
        
popd

kill_model_server
